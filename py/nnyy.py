# -*- coding: utf-8 -*-
# @Author  : Doubebly
# @Time    : 2025/5/29 22:07

import sys
import time
import requests
import re
import html
import base64
from urllib.parse import quote, urljoin
sys.path.append('..')
from base.spider import Spider


class Spider(Spider):
    def getName(self):
        return "Nunuyy"

    def init(self, extend):
        self.home_url = 'https://nnyy.la/'
        self.ua = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36"
        self.error_url = "https://sf1-cdn-tos.huoshanstatic.com/obj/media-fe/xgplayer_doc_video/mp4/xgplayer-demo-720p.mp4"
        
        # åˆ†ç±»æ˜ å°„
        self.cate_map = {
            '1': 'dianying',
            '2': 'dianshiju', 
            '3': 'zongyi',
            '4': 'dongman',
            '5': 'jilupian'
        }

    def getDependence(self):
        return []

    def isVideoFormat(self, url):
        pass

    def manualVideoCheck(self):
        pass

    def homeContent(self, filter):
        return {
            'class': [
                {'type_id': '0', 'type_name': 'é˜¿å¸ƒå½±é™¢'},
                {'type_id': '1', 'type_name': 'ç”µå½±'},
                {'type_id': '2', 'type_name': 'ç”µè§†å‰§'},
                {'type_id': '3', 'type_name': 'ç»¼è‰º'},
                {'type_id': '4', 'type_name': 'åŠ¨æ¼«'},
                {'type_id': '5', 'type_name': 'çºªå½•ç‰‡'}
            ],
            'filters': {
                '1': [
                    {'key': 'class', 'name': 'åˆ†ç±»', 'value': [
                        {'n': 'å…¨éƒ¨', 'v': ''}, {'n': 'å–œå‰§', 'v': 'å–œå‰§'}, {'n': 'çˆ±æƒ…', 'v': 'çˆ±æƒ…'},
                        {'n': 'åŠ¨ä½œ', 'v': 'åŠ¨ä½œ'}, {'n': 'ç§‘å¹»', 'v': 'ç§‘å¹»'}, {'n': 'å¥‡å¹»', 'v': 'å¥‡å¹»'},
                        {'n': 'æ‚¬ç–‘', 'v': 'æ‚¬ç–‘'}, {'n': 'çŠ¯ç½ª', 'v': 'çŠ¯ç½ª'}, {'n': 'å†’é™©', 'v': 'å†’é™©'},
                        {'n': 'ç¾éš¾', 'v': 'ç¾éš¾'}, {'n': 'ææ€–', 'v': 'ææ€–'}, {'n': 'æƒŠæ‚š', 'v': 'æƒŠæ‚š'},
                        {'n': 'å‰§æƒ…', 'v': 'å‰§æƒ…'}, {'n': 'æˆ˜äº‰', 'v': 'æˆ˜äº‰'}, {'n': 'å†å²', 'v': 'å†å²'},
                        {'n': 'ä¼ è®°', 'v': 'ä¼ è®°'}, {'n': 'æ­Œèˆ', 'v': 'æ­Œèˆ'}, {'n': 'æ­¦ä¾ ', 'v': 'æ­¦ä¾ '},
                        {'n': 'æƒ…è‰²', 'v': 'æƒ…è‰²'}, {'n': 'è¥¿éƒ¨', 'v': 'è¥¿éƒ¨'}, {'n': 'ç»å…¸', 'v': 'ç»å…¸'},
                        {'n': 'åŠ¨ç”»', 'v': 'åŠ¨ç”»'}, {'n': 'åŒæ€§', 'v': 'åŒæ€§'}, {'n': 'ç½‘ç»œç”µå½±', 'v': 'ç½‘ç»œç”µå½±'}
                    ]},
                    {'key': 'area', 'name': 'åœ°åŒº', 'value': [
                        {'n': 'å…¨éƒ¨', 'v': ''}, {'n': 'å¤§é™†', 'v': 'å¤§é™†'}, {'n': 'é¦™æ¸¯', 'v': 'é¦™æ¸¯'},
                        {'n': 'å°æ¹¾', 'v': 'å°æ¹¾'}, {'n': 'æ¬§ç¾', 'v': 'æ¬§ç¾'}, {'n': 'éŸ©å›½', 'v': 'éŸ©å›½'},
                        {'n': 'æ—¥æœ¬', 'v': 'æ—¥æœ¬'}, {'n': 'æ³•å›½', 'v': 'æ³•å›½'}, {'n': 'å¾·å›½', 'v': 'å¾·å›½'},
                        {'n': 'æ„å¤§åˆ©', 'v': 'æ„å¤§åˆ©'}, {'n': 'è¥¿ç­ç‰™', 'v': 'è¥¿ç­ç‰™'}, {'n': 'å°åº¦', 'v': 'å°åº¦'},
                        {'n': 'æ³°å›½', 'v': 'æ³°å›½'}, {'n': 'å…¶å®ƒ', 'v': 'å…¶å®ƒ'}
                    ]},
                    {'key': 'year', 'name': 'å¹´ä»£', 'value': [
                        {'n': 'å…¨éƒ¨', 'v': ''}, {'n': '2020å¹´ä»£', 'v': '2020å¹´ä»£'}, {'n': '2025', 'v': '2025'},
                        {'n': '2024', 'v': '2024'}, {'n': '2023', 'v': '2023'}, {'n': '2022', 'v': '2022'},
                        {'n': '2021', 'v': '2021'}, {'n': '2020', 'v': '2020'}, {'n': '2019', 'v': '2019'},
                        {'n': '2010å¹´ä»£', 'v': '2010å¹´ä»£'}, {'n': '2000å¹´ä»£', 'v': '2000å¹´ä»£'}, {'n': '90å¹´ä»£', 'v': '90å¹´ä»£'},
                        {'n': '80å¹´ä»£', 'v': '80å¹´ä»£'}, {'n': '70å¹´ä»£', 'v': '70å¹´ä»£'}, {'n': '60å¹´ä»£', 'v': '60å¹´ä»£'},
                        {'n': 'æ›´æ—©', 'v': 'æ›´æ—©'}
                    ]},
                    {'key': 'by', 'name': 'æ’åº', 'value': [
                        {'n': 'æŒ‰æ—¶é—´æ’åº', 'v': 'time'}, {'n': 'æŒ‰äººæ°”æ’åº', 'v': 'hits'}, {'n': 'æŒ‰è¯„åˆ†æ’åº', 'v': 'score'}
                    ]}
                ],
                '2': [
                    {'key': 'class', 'name': 'åˆ†ç±»', 'value': [
                        {'n': 'å…¨éƒ¨', 'v': ''}, {'n': 'å–œå‰§', 'v': 'å–œå‰§'}, {'n': 'å¶åƒ', 'v': 'å¶åƒ'},
                        {'n': 'çˆ±æƒ…', 'v': 'çˆ±æƒ…'}, {'n': 'è¨€æƒ…', 'v': 'è¨€æƒ…'}, {'n': 'å¤è£…', 'v': 'å¤è£…'},
                        {'n': 'å†å²', 'v': 'å†å²'}, {'n': 'ç„å¹»', 'v': 'ç„å¹»'}, {'n': 'è°æˆ˜', 'v': 'è°æˆ˜'},
                        {'n': 'å†é™©', 'v': 'å†é™©'}, {'n': 'éƒ½å¸‚', 'v': 'éƒ½å¸‚'}, {'n': 'ç§‘å¹»', 'v': 'ç§‘å¹»'},
                        {'n': 'å†›æ—…', 'v': 'å†›æ—…'}, {'n': 'æ­¦ä¾ ', 'v': 'æ­¦ä¾ '}, {'n': 'æ±Ÿæ¹–', 'v': 'æ±Ÿæ¹–'},
                        {'n': 'ç½ªæ¡ˆ', 'v': 'ç½ªæ¡ˆ'}, {'n': 'é’æ˜¥', 'v': 'é’æ˜¥'}, {'n': 'å®¶åº­', 'v': 'å®¶åº­'},
                        {'n': 'æˆ˜äº‰', 'v': 'æˆ˜äº‰'}, {'n': 'æ‚¬ç–‘', 'v': 'æ‚¬ç–‘'}, {'n': 'ç©¿è¶Š', 'v': 'ç©¿è¶Š'},
                        {'n': 'å®«å»·', 'v': 'å®«å»·'}, {'n': 'ç¥è¯', 'v': 'ç¥è¯'}, {'n': 'å•†æˆ˜', 'v': 'å•†æˆ˜'},
                        {'n': 'è­¦åŒª', 'v': 'è­¦åŒª'}, {'n': 'åŠ¨ä½œ', 'v': 'åŠ¨ä½œ'}, {'n': 'æƒŠæ‚š', 'v': 'æƒŠæ‚š'},
                        {'n': 'å‰§æƒ…', 'v': 'å‰§æƒ…'}, {'n': 'åŒæ€§', 'v': 'åŒæ€§'}, {'n': 'å¥‡å¹»', 'v': 'å¥‡å¹»'},
                        {'n': 'å¹´ä»£', 'v': 'å¹´ä»£'}
                    ]},
                    {'key': 'area', 'name': 'åœ°åŒº', 'value': [
                        {'n': 'å…¨éƒ¨', 'v': ''}, {'n': 'å¤§é™†', 'v': 'å¤§é™†'}, {'n': 'é¦™æ¸¯', 'v': 'é¦™æ¸¯'},
                        {'n': 'å°æ¹¾', 'v': 'å°æ¹¾'}, {'n': 'æ¬§ç¾', 'v': 'æ¬§ç¾'}, {'n': 'éŸ©å›½', 'v': 'éŸ©å›½'},
                        {'n': 'æ—¥æœ¬', 'v': 'æ—¥æœ¬'}, {'n': 'è‹±å›½', 'v': 'è‹±å›½'}, {'n': 'æ³°å›½', 'v': 'æ³°å›½'},
                        {'n': 'å…¶å®ƒ', 'v': 'å…¶å®ƒ'}
                    ]},
                    {'key': 'year', 'name': 'å¹´ä»£', 'value': [
                        {'n': 'å…¨éƒ¨', 'v': ''}, {'n': '2025', 'v': '2025'}, {'n': '2024', 'v': '2024'},
                        {'n': '2023', 'v': '2023'}, {'n': '2022', 'v': '2022'}, {'n': '2021', 'v': '2021'},
                        {'n': '2020', 'v': '2020'}, {'n': '2019', 'v': '2019'}, {'n': '2020å¹´ä»£', 'v': '2020å¹´ä»£'},
                        {'n': '2010å¹´ä»£', 'v': '2010å¹´ä»£'}, {'n': '2000å¹´ä»£', 'v': '2000å¹´ä»£'}, {'n': '90å¹´ä»£', 'v': '90å¹´ä»£'},
                        {'n': '80å¹´ä»£', 'v': '80å¹´ä»£'}, {'n': '70å¹´ä»£', 'v': '70å¹´ä»£'}, {'n': '60å¹´ä»£', 'v': '60å¹´ä»£'},
                        {'n': 'æ›´æ—©', 'v': 'æ›´æ—©'}
                    ]},
                    {'key': 'by', 'name': 'æ’åº', 'value': [
                        {'n': 'æŒ‰æ—¶é—´æ’åº', 'v': 'time'}, {'n': 'æŒ‰äººæ°”æ’åº', 'v': 'hits'}, {'n': 'æŒ‰è¯„åˆ†æ’åº', 'v': 'score'}
                    ]}
                ]
            }
        }

    def homeVideoContent(self):
        video_list = []
        try:
            res = requests.get(f'{self.home_url}dianying/', headers={"User-Agent": self.ua})
            html_text = res.text
            
            # ä½¿ç”¨555.pyçš„ç¨³å®šæå–é€»è¾‘
            li_pattern = r'<li>\s*<a href="([^"]+)"[^>]*class="thumbnail"[^>]*>.*?<img[^>]*data-src="([^"]+)"[^>]*>.*?<div class="note"><span>([^<]+)</span>.*?<h2><a[^>]*>([^<]+)</a>'
            matches = re.findall(li_pattern, html_text, re.DOTALL)
            
            for match in matches:
                url, pic, remarks, name = match
                vod_id = url.split('/')[-1].replace('.html', '')
                
                video_list.append({
                    'vod_id': vod_id,
                    'vod_name': html.unescape(name.strip()),
                    'vod_pic': pic,
                    'vod_remarks': remarks.strip()
                })
                
                if len(video_list) >= 20:
                    break
                    
        except:
            pass
            
        return {
            'list': video_list,
            'parse': 0,
            'jx': 0
        }

    def categoryContent(self, cid, page, filter, ext):
        video_list = []
        
        # è·å–åˆ†ç±»åç§°
        cate_name = self.cate_map.get(cid, 'dianying')
        
        # æ„å»ºURL - ä½¿ç”¨555.pyçš„ç®€å•é€»è¾‘
        if page == 1:
            url = f'{self.home_url}{cate_name}/'
        else:
            url = f'{self.home_url}{cate_name}/?page={page}'
        
        # å¦‚æœæœ‰ç­›é€‰æ¡ä»¶ï¼Œæ·»åŠ åˆ°URLä¸­
        if ext:
            params = []
            for key, value in ext.items():
                if value:
                    params.append(f'{key}={quote(value)}')
            if params:
                url = f'{url}?{"&".join(params)}'
        
        try:
            res = requests.get(url, headers={"User-Agent": self.ua})
            html_text = res.text
            
            # ä½¿ç”¨555.pyçš„ç¨³å®šæå–é€»è¾‘
            li_pattern = r'<li>\s*<a href="([^"]+)"[^>]*class="thumbnail"[^>]*>.*?<img[^>]*data-src="([^"]+)"[^>]*>.*?<div class="note"><span>([^<]+)</span>.*?<div class="countrie">.*?<h2><a[^>]*>([^<]+)</a>'
            matches = re.findall(li_pattern, html_text, re.DOTALL)
            
            for match in matches:
                url, pic, remarks, name = match
                vod_id = url.split('/')[-1].replace('.html', '')
                
                # å°è¯•æå–æ›´å¤šä¿¡æ¯ï¼ˆå¹´ä»½ã€åœ°åŒºï¼‰
                year = ''
                area = ''
                try:
                    # å°è¯•ä»countrieä¸­æå–å¹´ä»½å’Œåœ°åŒº
                    country_pattern = r'<div class="countrie">(.*?)</div>'
                    country_match = re.search(country_pattern, html_text[html_text.find(url):html_text.find(url)+500], re.DOTALL)
                    if country_match:
                        country_text = country_match.group(1)
                        # æå–å¹´ä»½å’Œåœ°åŒº
                        span_matches = re.findall(r'<span[^>]*>([^<]+)</span>', country_text)
                        if len(span_matches) >= 2:
                            year = span_matches[0]
                            area = span_matches[1]
                        elif len(span_matches) == 1:
                            year = span_matches[0]
                except:
                    pass
                
                video_list.append({
                    'vod_id': vod_id,
                    'vod_name': html.unescape(name.strip()),
                    'vod_pic': pic,
                    'vod_remarks': remarks.strip(),
                    'vod_year': year,
                    'vod_area': area
                })
                
        except:
            pass
        
        return {'list': video_list, 'parse': 0, 'jx': 0}

    def detailContent(self, did):
        ids = did[0]
        video_list = []
        
        try:
            # å°è¯•ä»ç”µå½±åˆ†ç±»è·å–è¯¦æƒ…é¡µ
            detail_url = f'{self.home_url}dianying/{ids}.html'
            res = requests.get(detail_url, headers={"User-Agent": self.ua})
            
            if res.status_code != 200:
                # å¦‚æœç”µå½±åˆ†ç±»æ²¡æœ‰ï¼Œå°è¯•å…¶ä»–åˆ†ç±»
                for cate_name in ['dianshiju', 'zongyi', 'dongman', 'jilupian']:
                    detail_url = f'{self.home_url}{cate_name}/{ids}.html'
                    res = requests.get(detail_url, headers={"User-Agent": self.ua})
                    if res.status_code == 200:
                        break
            
            html_text = res.text
            
            # æå–åŸºæœ¬ä¿¡æ¯
            # æ ‡é¢˜
            title_match = re.search(r'<h2[^>]*>([^<]+)<span>', html_text)
            if not title_match:
                title_match = re.search(r'<h1[^>]*>([^<]+)<span>', html_text)
            
            vod_name = html.unescape(title_match.group(1).strip()) if title_match else ids
            
            # æå–å¹´ä»½
            year_match = re.search(r'<span>\((\d{4})\)</span>', html_text)
            vod_year = year_match.group(1) if year_match else ''
            
            # å›¾ç‰‡
            pic_match = re.search(r'<img[^>]*data-src="([^"]+)"[^>]*alt="[^"]*"[^>]*>', html_text)
            vod_pic = pic_match.group(1) if pic_match else ''
            
            # å¯¼æ¼”
            director_match = re.search(r'å¯¼æ¼”[ï¼š:]<span[^>]*>(.*?)</span>', html_text, re.DOTALL)
            if director_match:
                director_text = director_match.group(1)
                director_names = re.findall(r'>([^<]+)</a>', director_text)
                vod_director = ','.join(director_names) if director_names else ''
            else:
                vod_director = ''
            
            # æ¼”å‘˜
            actor_match = re.search(r'ä¸»æ¼”[ï¼š:]<span[^>]*>(.*?)</span>', html_text, re.DOTALL)
            if actor_match:
                actor_text = actor_match.group(1)
                actor_names = re.findall(r'>([^<]+)</a>', actor_text)
                vod_actor = ','.join(actor_names) if actor_names else ''
            else:
                vod_actor = ''
            
            # ç±»å‹
            type_match = re.search(r'ç±»å‹[ï¼š:]<span[^>]*>(.*?)</span>', html_text, re.DOTALL)
            if type_match:
                type_text = type_match.group(1)
                type_names = re.findall(r'>([^<]+)</a>', type_text)
                vod_type = ','.join(type_names) if type_names else ''
            else:
                vod_type = ''
            
            # åœ°åŒº
            area_match = re.search(r'åˆ¶ç‰‡å›½å®¶/åœ°åŒº[ï¼š:]<span[^>]*>(.*?)</span>', html_text, re.DOTALL)
            if area_match:
                area_text = area_match.group(1)
                area_names = re.findall(r'>([^<]+)</a>', area_text)
                vod_area = ','.join(area_names) if area_names else ''
            else:
                vod_area = ''
            
            # ç®€ä»‹
            desc_match = re.search(r'å‰§æƒ…ç®€ä»‹[ï¼š:]<span[^>]*>([^<]+)</span>', html_text)
            vod_content = html.unescape(desc_match.group(1).strip()) if desc_match else ''
            
            # è¯„åˆ†
            rate_match = re.search(r'<span class="rate">([^<]+)</span>', html_text)
            vod_score = rate_match.group(1) if rate_match else ''
            
            # æ’­æ”¾æºå’Œå‰§é›† - å…³é”®ä¼˜åŒ–ï¼šæå–åŠ å¯†çš„æ’­æ”¾åœ°å€
            # æå–æ‰€æœ‰æ’­æ”¾æº
            source_pattern = r'<dt data-sid="(\d+)"[^>]*>([^<]+)</dt>'
            source_matches = re.findall(source_pattern, html_text)
            
            # æå–åŠ å¯†çš„URLå­—å…¸
            encrypted_dict = {}
            url_dict_pattern = r'urlDictionary\[(\d+)\]\[(\d+)\]\s*=\s*"([^"]+)"'
            url_dict_matches = re.findall(url_dict_pattern, html_text)
            
            for sid, nid, encrypted_url in url_dict_matches:
                if int(sid) not in encrypted_dict:
                    encrypted_dict[int(sid)] = {}
                encrypted_dict[int(sid)][int(nid)] = encrypted_url
            
            vod_play_from = []
            vod_play_url = []
            
            if source_matches:
                for sid_str, source_name in source_matches:
                    sid = int(sid_str)
                    
                    # æŸ¥æ‰¾è¯¥æºçš„æ‰€æœ‰å‰§é›†
                    episode_pattern = rf'data-sid="{sid}"[^>]*data-nid="(\d+)"[^>]*>.*?<a[^>]*>([^<]+)</a>'
                    episode_matches = re.findall(episode_pattern, html_text, re.DOTALL)
                    
                    if episode_matches:
                        vod_play_from.append(source_name)
                        episodes = []
                        for nid_str, episode_name in episode_matches:
                            nid = int(nid_str)
                            
                            # æ„å»ºæ’­æ”¾IDï¼šæ ¼å¼ä¸º sid|nid|encrypted_url|detail_url
                            if sid in encrypted_dict and nid in encrypted_dict[sid]:
                                encrypted_url = encrypted_dict[sid][nid]
                                play_id = f"{sid}|{nid}|{encrypted_url}|{detail_url}"
                            else:
                                play_id = f"{sid}|{nid}|{detail_url}"
                            
                            episodes.append(f"{episode_name.strip()}${play_id}")
                        
                        if episodes:
                            vod_play_url.append('#'.join(episodes))
            
            # å¦‚æœæ²¡æœ‰æå–åˆ°æ’­æ”¾æºï¼Œä½¿ç”¨é»˜è®¤å€¼
            if not vod_play_from:
                vod_play_from = ['é˜¿å¸ƒå½±é™¢']
                vod_play_url = [f"å°å¸ƒä¸çº¿è·¯${detail_url}"]
            
            # æ„å»ºè§†é¢‘ä¿¡æ¯
            video_info = {
                'type_name': vod_type,
                'vod_id': ids,
                'vod_name': vod_name,
                'vod_pic': vod_pic,
                'vod_year': vod_year,
                'vod_area': vod_area,
                'vod_remarks': f"è¯„åˆ†:{vod_score}" if vod_score else '',
                'vod_actor': vod_actor,
                'vod_director': vod_director,
                'vod_content': f"é˜¿å¸ƒä¸ºä½ ä»‹ç»ğŸ¤©ğŸ¤™{vod_content}",
                'vod_play_from': '$$$'.join(vod_play_from),
                'vod_play_url': '$$$'.join(vod_play_url)
            }
            
            video_list.append(video_info)
            
        except Exception as e:
            # å¦‚æœè¯¦æƒ…é¡µè§£æå¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªåŸºæœ¬çš„è§†é¢‘ä¿¡æ¯
            print(f"è§£æè¯¦æƒ…é¡µå¤±è´¥: {e}")
            pass
        
        return {"list": video_list, 'parse': 0, 'jx': 0}

    def searchContent(self, key, quick, page='1'):
        wd = key
        video_list = []
        
        try:
            search_url = f'{self.home_url}search?wd={quote(wd)}&page={page}'
            res = requests.get(search_url, headers={"User-Agent": self.ua})
            html_text = res.text
            
            # ä½¿ç”¨555.pyçš„ç¨³å®šæå–é€»è¾‘
            li_pattern = r'<li>\s*<a href="([^"]+)"[^>]*class="thumbnail"[^>]*>.*?<img[^>]*data-src="([^"]+)"[^>]*>.*?<div class="note"><span>([^<]+)</span>.*?<h2><a[^>]*>([^<]+)</a>'
            matches = re.findall(li_pattern, html_text, re.DOTALL)
            
            for match in matches:
                url, pic, remarks, name = match
                vod_id = url.split('/')[-1].replace('.html', '')
                
                video_list.append({
                    'vod_id': vod_id,
                    'vod_name': html.unescape(name.strip()),
                    'vod_pic': pic,
                    'vod_remarks': remarks.strip()
                })
                
        except:
            pass
        
        return {'list': video_list, 'parse': 0, 'jx': 0}

    def rc4_decrypt(self, encrypted_hex, key="i_love_you"):
        """RC4è§£å¯†å‡½æ•°ï¼Œä¸ç½‘é¡µä¸­çš„JavaScriptå®ç°ä¸€è‡´"""
        try:
            # å°†åå…­è¿›åˆ¶å­—ç¬¦ä¸²è½¬æ¢ä¸ºå­—èŠ‚
            encrypted_bytes = bytes.fromhex(encrypted_hex)
            
            # RC4ç®—æ³•å®ç°
            s = list(range(256))
            j = 0
            key_length = len(key)
            
            # KSA (Key-Scheduling Algorithm)
            for i in range(256):
                j = (j + s[i] + ord(key[i % key_length])) % 256
                s[i], s[j] = s[j], s[i]
            
            # PRGA (Pseudo-Random Generation Algorithm) å¹¶è§£å¯†
            i = j = 0
            decrypted_bytes = bytearray()
            
            for byte in encrypted_bytes:
                i = (i + 1) % 256
                j = (j + s[i]) % 256
                s[i], s[j] = s[j], s[i]
                key_byte = s[(s[i] + s[j]) % 256]
                decrypted_bytes.append(byte ^ key_byte)
            
            return decrypted_bytes.decode('utf-8', errors='ignore')
        except:
            return None

    def playerContent(self, flag, pid, vipFlags):
        play_url = self.error_url
        parse = 0  # é»˜è®¤ä¸è§£æï¼Œç›´æ¥æ’­æ”¾
        
        try:
            # è§£æpidæ ¼å¼: sid|nid|encrypted_url|detail_url æˆ– sid|nid|detail_url
            parts = pid.split('|')
            
            if len(parts) >= 4:
                # æ ¼å¼: sid|nid|encrypted_url|detail_url
                sid = parts[0]
                nid = parts[1]
                encrypted_url = parts[2]
                detail_url = parts[3]
                
                # å°è¯•ä½¿ç”¨RC4è§£å¯†
                decrypted_url = self.rc4_decrypt(encrypted_url)
                
                if decrypted_url and decrypted_url.startswith('http'):
                    play_url = decrypted_url
                    parse = 0  # ç›´æ¥æ’­æ”¾è§£å¯†åçš„URL
                else:
                    # è§£å¯†å¤±è´¥ï¼Œè¿”å›è¯¦æƒ…é¡µåœ°å€è¿›è¡Œå—…æ¢
                    play_url = detail_url
                    parse = 1  # éœ€è¦è§£æ
                    
            elif len(parts) == 3:
                # æ ¼å¼: sid|nid|detail_url
                detail_url = parts[2]
                play_url = detail_url
                parse = 1  # éœ€è¦è§£æ
                
            else:
                # ä¸æ˜¯æ ‡å‡†æ ¼å¼ï¼Œå¯èƒ½æ˜¯ç›´æ¥çš„è¯¦æƒ…é¡µURL
                if pid.startswith('http'):
                    play_url = pid
                    parse = 1  # éœ€è¦è§£æ
                else:
                    # å¯èƒ½æ˜¯è§†é¢‘IDï¼Œå°è¯•æ„å»ºè¯¦æƒ…é¡µURL
                    play_url = f'{self.home_url}dianying/{pid}.html'
                    parse = 1  # éœ€è¦è§£æ
                    
        except Exception as e:
            print(f"æ’­æ”¾åœ°å€è§£æå¤±è´¥: {e}")
            # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨è¯¦æƒ…é¡µåœ°å€
            if 'http' in pid:
                play_url = pid
                parse = 1
            else:
                play_url = self.error_url
                parse = 0
        
        h2 = {
            "User-Agent": self.ua,
            "Referer": self.home_url,
            "Accept": "*/*",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive"
        }
        
        return {"url": play_url, "header": h2, "parse": parse, "jx": 0}

    def localProxy(self, params):
        pass

    def destroy(self):
        return 'æ­£åœ¨Destroy'

if __name__ == '__main__':
    pass